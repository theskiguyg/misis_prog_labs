# –õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ3


## –ó–∞–¥–∞–Ω–∏–µ A -> `src/lib/text.py`
## –†–µ–∞–ª–∏–∑—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –º–æ–¥—É–ª–µ `src/lib/text.py`. 
## –î–∞–ª–µ–µ –≤—ã–≤–æ–∂—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ `src/lab03/A.py`. 

### normalize

```python
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if not(isinstance(text, str)):
        return TypeError
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('–Å', "–ï").replace('—ë', '–µ')
    text = text.replace('\t', ' ').replace('\r', ' ').replace('\n', ' ')
    text = ' '.join(text.split())
    return text.strip()
```

### tokenize

```python
def tokenize(text: str) -> list[str]:
    if not(isinstance(text, str)):
        return TypeError
    pattern = r"\w+(?:-\w+)*"
    string = text
    tokens = finditer(pattern, string)
    return [i.group() for i in tokens]
```

### count_freq

```python
def count_freq(tokens: list[str]) -> dict[str, int]:
    if not(isinstance(tokens, list)):
        return TypeError
    freq = {}
    for i in tokens:
        if i in freq:
            freq[i] += 1
        else:
            freq[i] = 1
    return freq
```

### top_n

```python
def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    return sorted(freq.items(), key=lambda x: (-x[1], x[0]))[:n]
```

### –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ `src/lab03/A.py`.
### –ó–∞–ø—É—Å–∫ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ `python3 -m src.lab03.A`. 

```python
from src.lib.text import normalize, tokenize, top_n, count_freq


print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", yo2e=True))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))


print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))


print(top_n(count_freq(["a", "b", "a", "c", "b", "a"]), n=2))
print(top_n(count_freq(["bb", "aa", "bb", "aa", "cc"]), n=2))
```

![–í—ã–≤–æ–¥ 3.–ê](../../images/lab03/A.png)

## –ó–∞–¥–∞–Ω–∏–µ B -> `src/lab03/B_text_stats`

### –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ stdin `src/lab03/input.txt`, –≤—ã–∑–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ `src/lib/text.py`, –≤—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É. 
### –ó–∞–ø—É—Å–∫ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞: `python3 -m src.lab03.B_text_stats < src/lab03/input.txt`. 

```python
import sys
from src.lib.text import count_freq, top_n, normalize, tokenize

text = sys.stdin.read()

tokens = tokenize(text=normalize(text=text))
top = top_n(count_freq(tokens))

print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(set(tokens))}")
print("–¢–æ–ø-5:")
for w, c in top:
    print(f"{w}:{c}")
```

![–í—ã–≤–æ–¥ 3.B](../../images/lab03/B_text_stats.png)